{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('clean_datasets/books.csv')\n",
    "df_ratings = pd.read_csv('clean_datasets/ratings.csv')\n",
    "df_tags = pd.read_csv('clean_datasets/tags.csv')\n",
    "df_to_read = pd.read_csv('clean_datasets/to_read.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Modelo Filtrado Colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el dataframe con las calificaciones de usuarios\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "# Crear el modelo SVD\n",
    "svd_model = SVD(n_factors=100, n_epochs=40, lr_all=0.01, reg_all=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación modelo filtrado colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo usando validación cruzada\n",
    "cross_validate(svd_model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Filtrado Basado en Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un subconjunto de datos con solo algunas columnas\n",
    "subset_books = df_books[['book_id', 'title', 'authors', 'average_rating', 'original_publication_year']]\n",
    "subset_tags = df_tags[['tag_name', 'book_id']]\n",
    "\n",
    "# Combinar los tags en una sola fila por libro\n",
    "subset_tags = subset_tags.groupby('book_id')['tag_name'].apply(' '.join).reset_index()\n",
    "\n",
    "# Combinar los datos de libros y tags\n",
    "subset_books_profile = pd.merge(subset_books, subset_tags, on='book_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesar datos para el entrenamiento y evaluación de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(books_df, author_weight, tag_weight):\n",
    "    \n",
    "    # Crear embeddings de autores\n",
    "    unique_authors = books_df['authors'].unique()\n",
    "    author_to_index = {author: idx for idx, author in enumerate(unique_authors)}\n",
    "    \n",
    "    # Representar autores como un vector esparcido\n",
    "    author_embeddings = np.zeros((len(books_df), len(unique_authors)))\n",
    "    for i, author in enumerate(books_df['authors']):\n",
    "        author_embeddings[i, author_to_index[author]] = 1.0\n",
    "    \n",
    "    # Escalar el peso del autor\n",
    "    author_embeddings *= author_weight\n",
    "    \n",
    "    # Procesar las etiquetas usando TF-IDF y escalar el peso de las etiquetas\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tags_tfidf_matrix = tfidf.fit_transform(books_df['tag_name']).toarray() * tag_weight\n",
    "    \n",
    "    # Normalizar las fechas de publicación\n",
    "    books_df['year_normalized'] = (books_df['original_publication_year'] - books_df['original_publication_year'].min()) / (\n",
    "                                  books_df['original_publication_year'].max() - books_df['original_publication_year'].min())\n",
    "    \n",
    "    # Concatenar todas las características en una única matriz de características\n",
    "    X = np.hstack([\n",
    "        author_embeddings,  # Embeddings de autores ponderados\n",
    "        books_df[['average_rating', 'year_normalized']].values,  # Calificaciones y años\n",
    "        tags_tfidf_matrix  # Etiquetas (tags) con TF-IDF ponderados\n",
    "    ])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del modelo siamesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(input_shape):\n",
    "    # Modelo base que compartirá pesos\n",
    "    base_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Entradas para dos libros\n",
    "    input_1 = tf.keras.layers.Input(shape=input_shape)\n",
    "    input_2 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Extraemos las representaciones usando el modelo base\n",
    "    encoded_1 = base_model(input_1)\n",
    "    encoded_2 = base_model(input_2)\n",
    "\n",
    "    # Cálculo de la distancia euclidiana entre las dos representaciones\n",
    "    distance = tf.keras.layers.Lambda(lambda embeddings: tf.reduce_sum(tf.square(embeddings[0] - embeddings[1]), axis=1))([encoded_1, encoded_2])\n",
    "    \n",
    "    # Modelo completo con dos entradas y una salida\n",
    "    siamese_model = tf.keras.Model(inputs=[input_1, input_2], outputs=distance)\n",
    "    \n",
    "    siamese_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return siamese_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtros para recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_first_book_in_series(title):\n",
    "    return bool(re.search(r'(#1)', title)) or not re.search(r'#\\d', title)\n",
    "\n",
    "# Filtrar duplicados o títulos que son colecciones de series, incluyendo boxsets de trilogías o colecciones\n",
    "def filter_duplicate_titles(recommended_indices, books_df):\n",
    "    filtered_recommendations = []\n",
    "    titles_seen = set()\n",
    "\n",
    "    for idx in recommended_indices:\n",
    "        title = books_df.iloc[idx]['title']\n",
    "        # Si el título no incluye \"boxset\" o algo similar, y no se ha visto antes\n",
    "        if \"boxset\" not in title.lower() and \"complete collection\" not in title.lower() and title not in titles_seen:\n",
    "            filtered_recommendations.append(idx)\n",
    "            titles_seen.add(title)\n",
    "    \n",
    "    return filtered_recommendations\n",
    "\n",
    "def apply_diversity_filter(recommended_indices, books_df, max_books_per_author=2, top_n=5, prioritize_first_books=True, require_independent_books=2):\n",
    "    filtered_recommendations = []\n",
    "    authors_seen = {}\n",
    "    independent_books_found = 0\n",
    "\n",
    "    for idx in recommended_indices:\n",
    "        author = books_df.iloc[idx]['authors']\n",
    "        title = books_df.iloc[idx]['title']\n",
    "        \n",
    "        # Comprobar si el autor ya tiene demasiadas recomendaciones\n",
    "        if authors_seen.get(author, 0) < max_books_per_author:\n",
    "            if not prioritize_first_books or is_first_book_in_series(title):\n",
    "                filtered_recommendations.append(idx)\n",
    "                authors_seen[author] = authors_seen.get(author, 0) + 1\n",
    "                \n",
    "                # Identificar libros independientes\n",
    "                if is_first_book_in_series(title):\n",
    "                    independent_books_found += 1\n",
    "\n",
    "        # Terminar cuando tengamos suficientes recomendaciones\n",
    "        if len(filtered_recommendations) >= top_n:\n",
    "            break\n",
    "\n",
    "    # Garantizar al menos un par de libros independientes\n",
    "    if independent_books_found < require_independent_books:\n",
    "        for idx in recommended_indices:\n",
    "            title = books_df.iloc[idx]['title']\n",
    "            if is_first_book_in_series(title):\n",
    "                filtered_recommendations.append(idx)\n",
    "                independent_books_found += 1\n",
    "            if independent_books_found >= require_independent_books:\n",
    "                break\n",
    "\n",
    "    return filtered_recommendations[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buscar el índice por book_id\n",
    "def get_book_index_by_id(book_id, books_df):\n",
    "    \"\"\"\n",
    "    Dado un book_id, devuelve el índice correspondiente en el DataFrame.\n",
    "    \"\"\"\n",
    "    # Buscar el índice en el DataFrame que corresponda con el book_id\n",
    "    try:\n",
    "        return books_df.loc[books_df['book_id'] == book_id].index[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"El book_id {book_id} no se encontró en el DataFrame\")\n",
    "\n",
    "# Función para calcular recomendaciones en lotes (batch) usando la GPU\n",
    "def get_recommendations_with_batches(book_id, X, model, books_df, batch_size, top_n, max_books_per_author):\n",
    "    book_index = get_book_index_by_id(book_id, books_df)\n",
    "    \n",
    "    book_vector = X[book_index].reshape(1, -1)\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        batch_size_actual = len(batch)\n",
    "        \n",
    "        book_batch = np.tile(book_vector, (batch_size_actual, 1))\n",
    "        batch_distances = model.predict([book_batch, batch])\n",
    "        \n",
    "        distances.extend(batch_distances)\n",
    "\n",
    "    distances = np.array(distances).flatten()\n",
    "    recommended_indices = np.argsort(distances)[:top_n * 2]\n",
    "\n",
    "    # Filtrar duplicados (por ejemplo, boxsets) y aplicar el filtro de diversidad\n",
    "    filtered_recommendations = filter_duplicate_titles(recommended_indices, books_df)\n",
    "    final_recommendations = apply_diversity_filter(filtered_recommendations, books_df, max_books_per_author, top_n, prioritize_first_books=True, require_independent_books=2)\n",
    "    \n",
    "    return final_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar y preprocesar los datos\n",
    "X = preprocess_data(subset_books_profile, author_weight=0.05, tag_weight=3.0)\n",
    "\n",
    "# Dividir datos para entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construir y entrenar el modelo siamesa\n",
    "content_model = build_siamese_model(X_train.shape[1])\n",
    "with tf.device('/GPU:0'):  # Aseguramos que el entrenamiento se haga en la GPU\n",
    "    content_model.fit([X_train, X_train], np.zeros(len(X_train)), epochs=10, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Obtener recomendaciones usando el book_id en lugar del índice\n",
    "book_id = 2767052  # Ejemplo de un book_id\n",
    "with tf.device('/GPU:0'):  # Usamos la GPU para la inferencia\n",
    "    recommended_books_indices = get_recommendations_with_batches(\n",
    "        book_id, X, content_model, subset_books_profile, batch_size=256, top_n=10, max_books_per_author=2\n",
    "    )\n",
    "\n",
    "# Mostrar libros recomendados\n",
    "recommended_books = subset_books_profile.iloc[recommended_books_indices]['title'].values\n",
    "print(\"Recomendaciones por book_id:\")\n",
    "for book in recommended_books:\n",
    "    print(\"-\", book)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Hibrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
