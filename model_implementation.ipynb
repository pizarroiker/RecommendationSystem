{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('clean_datasets/books.csv')\n",
    "df_ratings = pd.read_csv('clean_datasets/ratings.csv')\n",
    "df_to_read = pd.read_csv('clean_datasets/to_read.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesar datos para el entrenamiento y evaluación del modelo\n",
    "\n",
    "La siguiente función preprocesa los datos de libros, generando una matriz de características que se utilizará en el modelo de recomendaciones. Este proceso incluye la creación de embeddings para los autores, la transformación de etiquetas de los libros usando TF-IDF, y la normalización de las fechas de publicación.\n",
    "\n",
    "#### Argumentos:\n",
    "- `books_df (pd.DataFrame)`: DataFrame que contiene la información de los libros, como título, autores, etiquetas y fechas de publicación.\n",
    "- `tag_weight (float)`: Un factor de ponderación para ajustar la importancia de las etiquetas en la matriz de características. Por defecto, se establece en 1.4.\n",
    "\n",
    "#### Devuelve:\n",
    "- `X (np.array)`: Matriz de características de los libros que contiene las siguientes representaciones concatenadas:\n",
    "    - **Embeddings de los autores**: Representaciones binarizadas de los autores.\n",
    "    - **Valoración promedio del libro**: `average_rating`.\n",
    "    - **Año de publicación normalizado**: Una versión escalada del año de publicación.\n",
    "    - **Matriz TF-IDF de las etiquetas (tags)**: Representación numérica de las etiquetas asociadas a cada libro, ajustada por el `tag_weight`.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **Embeddings de autores**:\n",
    "   - Se crean embeddings binarios para los autores de los libros. Cada autor se asigna a una posición única en el vector y se le asigna un valor de 1 si el autor está asociado al libro.\n",
    "\n",
    "2. **Preprocesamiento de etiquetas (tags)**:\n",
    "   - Las etiquetas de los libros (`tag_name`) se procesan utilizando el modelo de TF-IDF (`TfidfVectorizer`), lo que genera una matriz TF-IDF que asigna un peso a cada etiqueta según su frecuencia relativa. El peso de las etiquetas se escala mediante el parámetro `tag_weight`.\n",
    "\n",
    "3. **Normalización de fechas de publicación**:\n",
    "   - El año de publicación original de cada libro se normaliza a un rango entre 0 y 1. Esto ayuda a estandarizar los valores en la matriz de características, asegurando que las fechas no dominen las demás características.\n",
    "\n",
    "4. **Concatenación de características**:\n",
    "   - Finalmente, se concatenan las siguientes matrices en una matriz de características completa (`X`):\n",
    "     - Embeddings de los autores.\n",
    "     - Valoración promedio del libro y año normalizado.\n",
    "     - Matriz TF-IDF de etiquetas, escalada por el peso de las etiquetas.\n",
    "\n",
    "Este preprocesamiento asegura que las características estén en un formato adecuado para ser utilizadas por el modelo de recomendaciones, y captura información relevante sobre los libros, incluyendo autores, popularidad, fecha de publicación y etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(books_df, tag_weight=1.4):\n",
    "    # Crear embeddings de autores\n",
    "    unique_authors = books_df['authors'].unique()\n",
    "    author_to_index = {author: idx for idx, author in enumerate(unique_authors)}\n",
    "    author_embeddings = np.zeros((len(books_df), len(unique_authors)))\n",
    "    \n",
    "    for i, author in enumerate(books_df['authors']):\n",
    "        author_embeddings[i, author_to_index[author]] = 1.0\n",
    "        \n",
    "    # Procesar etiquetas con TF-IDF\n",
    "    books_df['tag_name'].fillna('', inplace=True)\n",
    "    tfidf = TfidfVectorizer(stop_words=None)\n",
    "    tags_tfidf_matrix = tfidf.fit_transform(books_df['tag_name']).toarray() * tag_weight\n",
    "    \n",
    "    # Normalizar las fechas de publicación\n",
    "    books_df['year_normalized'] = (books_df['original_publication_year'] - books_df['original_publication_year'].min()) / (\n",
    "        books_df['original_publication_year'].max() - books_df['original_publication_year'].min())\n",
    "    \n",
    "    # Concatenar todas las características (embeddings de autores, calificaciones, fechas y TF-IDF de etiquetas)\n",
    "    X = np.hstack([\n",
    "        author_embeddings,\n",
    "        books_df[['average_rating', 'year_normalized']].values,\n",
    "        tags_tfidf_matrix\n",
    "    ])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construcción de la red neuronal siamesa\n",
    "\n",
    "La siguiente función construye un modelo Siamese utilizando una red neuronal densa que compara dos entradas (representando dos libros) y calcula la distancia entre sus representaciones (embeddings). Este modelo es útil en tareas como recomendaciones, donde se quiere medir la similitud entre dos elementos.\n",
    "\n",
    "#### Argumentos:\n",
    "- `input_shape (tuple)`: Dimensión de la entrada de características de cada libro. Es un `tuple` que especifica el número de características que describen cada libro.\n",
    "\n",
    "#### Devuelve:\n",
    "- `tf.keras.Model`: Un modelo Siamese de Keras con dos entradas y una salida. La salida es la distancia euclidiana entre los embeddings de las dos entradas (libros). El modelo se compila con el optimizador Adam y usa el error cuadrático medio (`mean_squared_error`) como función de pérdida.\n",
    "\n",
    "#### Descripción:\n",
    "\n",
    "1. **Modelo base compartido**: \n",
    "   - El modelo `base_model` es una red secuencial de tres capas densas con activación ReLU. Este modelo extrae las representaciones de los libros en un espacio de características de menor dimensión. El modelo es compartido entre ambas entradas, lo que significa que los pesos de las capas son iguales para ambas entradas.\n",
    "\n",
    "2. **Entradas del modelo**: \n",
    "   - La función toma dos entradas (`input_1` y `input_2`), que representan dos libros diferentes, cada uno con las mismas características (de tamaño `input_shape`).\n",
    "\n",
    "3. **Representación de las entradas**: \n",
    "   - Cada entrada pasa por el mismo modelo base (`base_model`) para obtener sus respectivas representaciones (`encoded_1` y `encoded_2`).\n",
    "\n",
    "4. **Cálculo de la distancia**: \n",
    "   - La distancia entre las dos representaciones es calculada usando la distancia euclidiana. Esto se realiza restando los embeddings de los dos libros y sumando el cuadrado de las diferencias por cada dimensión.\n",
    "\n",
    "5. **Compilación del modelo**:\n",
    "   - El modelo se compila con el optimizador Adam y se usa el error cuadrático medio (`mean_squared_error`) como la función de pérdida. Además, el modelo mide el error absoluto medio (`mae`) como métrica durante el entrenamiento.\n",
    "\n",
    "Este enfoque permite entrenar el modelo para aprender las representaciones de los libros en un espacio común y medir su similitud a través de la distancia euclidiana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(input_shape):\n",
    "    # Modelo base que compartirá pesos\n",
    "    base_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Entradas para dos libros\n",
    "    input_1 = tf.keras.layers.Input(shape=input_shape)\n",
    "    input_2 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Extraemos las representaciones usando el modelo base\n",
    "    encoded_1 = base_model(input_1)\n",
    "    encoded_2 = base_model(input_2)\n",
    "\n",
    "    # Cálculo de la distancia euclidiana entre las dos representaciones\n",
    "    distance = tf.keras.layers.Lambda(lambda embeddings: tf.reduce_sum(tf.square(embeddings[0] - embeddings[1]), axis=1))([encoded_1, encoded_2])\n",
    "    \n",
    "    # Modelo completo con dos entradas y una salida\n",
    "    siamese_model = tf.keras.Model(inputs=[input_1, input_2], outputs=distance)\n",
    "    \n",
    "    # Configurar el optimizador Adam con la precisión mixta\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    siamese_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filtros para recomendaciones\n",
    "\n",
    "Estas funciones se encargan de filtrar las recomendaciones de libros para evitar duplicados, colecciones, y mejorar la diversidad de las recomendaciones, controlando el número de libros del mismo autor y priorizando los primeros libros de series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función: `is_first_book_in_series`\n",
    "\n",
    "Esta función determina si un libro es el primer volumen de una serie.\n",
    "\n",
    "##### Argumentos:\n",
    "- `title (str)`: El título del libro, que potencialmente contiene un indicador de su posición en una serie (por ejemplo, \"#1\").\n",
    "\n",
    "##### Retorna:\n",
    "- `bool`: \n",
    "   - `True` si el libro es el primer volumen de una serie (por ejemplo, tiene \"#1\" en el título).\n",
    "   - `False` si el título contiene un número de volumen diferente (por ejemplo, \"#2\" o \"#13\").\n",
    "   - Si no se encuentra un número en el título, se asume que el libro es el primero de la serie por defecto.\n",
    "\n",
    "##### Descripción:\n",
    "- Usa expresiones regulares para buscar la indicación de \"volumen 1\" en el título de un libro.\n",
    "- Se excluyen los libros que tienen otros números como \"#13\", \"#14\", o rangos como \"#1-5\" para evitar confusiones con otros volúmenes de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_first_book_in_series(title):\n",
    "    if re.search(r'#1\\b(?![\\.-])', title):\n",
    "        return True\n",
    "    elif re.search(r'#\\d+', title):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función: `filter_duplicate_titles`\n",
    "\n",
    "Filtra títulos duplicados o aquellos que pertenecen a colecciones, como \"box sets\", trilogías, o ediciones completas.\n",
    "\n",
    "##### Argumentos:\n",
    "- `recommended_indices (list)`: Lista de índices de libros recomendados.\n",
    "- `books_df (pd.DataFrame)`: DataFrame que contiene la información de los libros, incluyendo los títulos.\n",
    "\n",
    "##### Retorna:\n",
    "- `list`: Una lista filtrada de índices de libros, excluyendo aquellos que tienen palabras clave que indican colecciones o duplicados (por ejemplo, \"box set\", \"trilogy\", \"omnibus\").\n",
    "\n",
    "##### Descripción:\n",
    "- La función utiliza una lista de palabras clave asociadas con colecciones o ediciones especiales (por ejemplo, \"box set\", \"complete collection\").\n",
    "- Para cada libro recomendado, verifica si su título contiene alguna de estas palabras clave y filtra los títulos que coincidan, devolviendo solo los libros individuales que no son colecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicate_titles(recommended_indices, books_df):\n",
    "    palabras_clave = [\"box set\", \"boxset\", \"complete collection\", \"boxed set\", \"omnibus\", \"trilogy\", \"quartet\", \"quintet\"]\n",
    "    filtered_recommendations = []\n",
    "\n",
    "    for idx in recommended_indices:\n",
    "        title = books_df.iloc[idx]['title']\n",
    "        if not any(keyword in title.lower() for keyword in palabras_clave):\n",
    "            filtered_recommendations.append(idx)\n",
    "\n",
    "    return filtered_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función: `apply_diversity_filter`\n",
    "\n",
    "Aplica un filtro de diversidad para asegurar que las recomendaciones no incluyan demasiados libros del mismo autor, y prioriza los primeros libros de las series.\n",
    "\n",
    "##### Argumentos:\n",
    "- `recommended_indices (list)`: Lista de índices de libros recomendados.\n",
    "- `books_df (pd.DataFrame)`: DataFrame con la información de los libros, incluyendo autores y títulos.\n",
    "- `book_id (int)`: ID del libro original sobre el cual se basó la recomendación, que debe ser excluido de los resultados.\n",
    "\n",
    "##### Retorna:\n",
    "- `list`: Una lista filtrada de índices que asegura una mayor diversidad en los autores y prioriza los primeros libros de las series.\n",
    "\n",
    "##### Descripción:\n",
    "- La función elimina el libro sobre el cual se basaron las recomendaciones de la lista de resultados.\n",
    "- Usa un diccionario para llevar un registro del número de recomendaciones por autor, permitiendo hasta un máximo de 3 libros por autor.\n",
    "- Además, prioriza la recomendación de los primeros libros de una serie utilizando la función `is_first_book_in_series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_diversity_filter(recommended_indices, books_df, book_id):\n",
    "    filtered_recommendations = []\n",
    "    authors_seen = {}\n",
    "\n",
    "    # Eliminar titulo sobre el que se hizo la recomendación\n",
    "    recommended_indices = [idx for idx in recommended_indices if idx != book_id]\n",
    "\n",
    "    for idx in recommended_indices:\n",
    "        author = books_df.iloc[idx]['authors']\n",
    "        title = books_df.iloc[idx]['title']\n",
    "\n",
    "        if authors_seen.get(author, 0) < 3:\n",
    "            if is_first_book_in_series(title):\n",
    "                filtered_recommendations.append(idx)\n",
    "                authors_seen[author] = authors_seen.get(author, 0) + 1\n",
    "\n",
    "    return filtered_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Obtención de recomendaciones\n",
    "\n",
    "Estas funciones permiten obtener el índice de un libro en un `DataFrame` y generar recomendaciones de libros similares utilizando un modelo Siamese para calcular distancias entre representaciones de libros (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función: `get_book_index_by_id`\n",
    "\n",
    "Esta función devuelve el índice de un libro en el `DataFrame` de libros utilizando su `book_id`.\n",
    "\n",
    "##### Argumentos:\n",
    "- `book_id (int)`: ID único del libro que se desea buscar.\n",
    "- `books_df (pd.DataFrame)`: DataFrame que contiene la información de los libros.\n",
    "\n",
    "##### Retorna:\n",
    "- `int`: El índice del libro en el `DataFrame`.\n",
    "\n",
    "##### Excepciones:\n",
    "- `ValueError`: Se lanza si el `book_id` proporcionado no se encuentra en el `DataFrame`.\n",
    "\n",
    "##### Descripción:\n",
    "- La función busca el libro con el `book_id` especificado en el `DataFrame`. Si no encuentra el libro, lanza una excepción `ValueError` con un mensaje indicando que no se pudo encontrar el libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_index_by_id(book_id, books_df):\n",
    "    try:\n",
    "        return books_df.loc[books_df['book_id'] == book_id].index[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"El book_id {book_id} no se encontró en el DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función: `get_recommendations_with_batches`\n",
    "\n",
    "Esta función genera una lista de recomendaciones de libros similares, calculando las distancias entre el libro dado y otros libros en lotes (batches), utilizando un modelo Siamese.\n",
    "\n",
    "##### Argumentos:\n",
    "- `book_id (int)`: ID del libro base para el cual se desean generar recomendaciones.\n",
    "- `X (np.array)`: Matriz de características de los libros.\n",
    "- `model (tf.keras.Model)`: El modelo Siamese que se utiliza para calcular las distancias entre los libros.\n",
    "- `books_df (pd.DataFrame)`: DataFrame que contiene la información de los libros.\n",
    "- `batch_size (int)`: Tamaño de los lotes en los que se procesan las características de los libros para calcular las distancias.\n",
    "- `rating (float)`: Calificación dada por el usuario al libro base, que se usa para ponderar las distancias.\n",
    "- `top_n (int)`: Número de recomendaciones que se desean devolver.\n",
    "\n",
    "##### Retorna:\n",
    "- `list`: Una lista de tuplas que contiene el `book_id` del libro recomendado y la distancia calculada entre el libro base y el libro recomendado.\n",
    "\n",
    "##### Descripción:\n",
    "1. **Obtener índice del libro base**:\n",
    "   - La función comienza obteniendo el índice del libro base en el `DataFrame` de libros mediante la función `get_book_index_by_id`.\n",
    "\n",
    "2. **Cálculo de distancias**:\n",
    "   - Se extrae el vector de características del libro base y se calcula la distancia entre este libro y los otros libros en la matriz `X` utilizando el modelo Siamese.\n",
    "   - El cálculo de distancias se realiza en lotes (`batches`) para mejorar la eficiencia cuando hay muchos libros en el `DataFrame`. \n",
    "   - Las distancias se ponderan inversamente al rating que el usuario ha dado al libro base (a menor rating, mayor peso en la distancia).\n",
    "\n",
    "3. **Filtrado de recomendaciones**:\n",
    "   - Tras calcular las distancias, los índices de los libros recomendados se ordenan en función de la distancia (de menor a mayor).\n",
    "   - Se aplican dos filtros adicionales:\n",
    "     - **Filtro de duplicados**: Se eliminan libros que son colecciones o tienen títulos duplicados usando `filter_duplicate_titles`.\n",
    "     - **Filtro de diversidad**: Se asegura la diversidad de autores en las recomendaciones usando `apply_diversity_filter`, que limita el número de libros recomendados por autor.\n",
    "\n",
    "4. **Formato de salida**:\n",
    "   - La función devuelve una lista de las `top_n` recomendaciones en forma de tuplas, donde cada tupla contiene el `book_id` del libro recomendado y su distancia respecto al libro base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_with_batches(book_id, X, model, books_df, batch_size, rating, top_n):\n",
    "\n",
    "    book_index = get_book_index_by_id(book_id, books_df)\n",
    "    book_vector = X[book_index].reshape(1, -1)\n",
    "    distances = []\n",
    "    weight = 1.0/rating\n",
    "\n",
    "    # Calcular distancias por lotes\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        batch_size_actual = len(batch)\n",
    "        book_batch = np.tile(book_vector, (batch_size_actual, 1))\n",
    "        batch_distances = model.predict([book_batch, batch])\n",
    "        distances.extend(batch_distances*weight)\n",
    "\n",
    "    distances = np.array(distances).flatten()\n",
    "    recommended_indices = distances.argsort()\n",
    "\n",
    "    # Aplicar filtros de duplicados y diversidad\n",
    "    filtered_recommendations = filter_duplicate_titles(recommended_indices, books_df)\n",
    "    final_recommendations = apply_diversity_filter(filtered_recommendations, books_df, book_id=book_index)\n",
    "\n",
    "    # Crear lista de tuplas (book_id, distancia)\n",
    "    recommendations_with_distances = [(books_df.iloc[idx]['book_id'], distances[idx]) for idx in final_recommendations]\n",
    "\n",
    "    return recommendations_with_distances[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Proceso completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "249/249 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 2/10\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 3/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 4/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 5/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 6/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 7/10\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 8/10\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 9/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n",
      "Epoch 10/10\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - mae: 0.0000e+00 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preprocesar los datos\n",
    "X = preprocess_data(df_books, tag_weight=1.4)\n",
    "\n",
    "# Construir y entrenar el modelo siamesa\n",
    "content_model = build_siamese_model(X.shape[1])\n",
    "\n",
    "# Entrenar el modelo asegurando que se utilice la GPU\n",
    "with tf.device('/GPU:0'):  \n",
    "    content_model.fit([X, X], np.zeros(len(X)), epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "33/33 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar las valoraciones de un usuario específico\n",
    "user_ratings = df_ratings[df_ratings['user_id'] == 1234]\n",
    "\n",
    "# Obtener los libros calificados por el usuario\n",
    "rated_books = user_ratings[['book_id', 'rating']]\n",
    "\n",
    "# Realizar recomendaciones para cada libro calificado\n",
    "recommended_books = []\n",
    "\n",
    "for _, row in rated_books.iterrows():\n",
    "    book_id = row['book_id']\n",
    "    rating = row['rating']\n",
    "    with tf.device('/GPU:0'):\n",
    "        recommended_books_indices = get_recommendations_with_batches(\n",
    "            book_id, X, content_model, df_books, 1048, rating, top_n=10\n",
    "        )\n",
    "    recommended_books.extend(recommended_books_indices)\n",
    "\n",
    "# Preparar las recomendaciones en el formato correcto\n",
    "recommended_books = pd.merge(pd.DataFrame(recommended_books, columns=['book_id', 'distance']), df_books, on='book_id')\n",
    "recommended_books = recommended_books.sort_values('distance').drop_duplicates(subset=['book_id'], keep='first')\n",
    "recommended_books = recommended_books[~recommended_books['book_id'].isin(user_ratings['book_id'])]\n",
    "recommended_books = recommended_books['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendaciones para el usuario 1:\n",
      "- Hide and Seek\n",
      "- Suzanne's Diary for Nicholas\n",
      "- 1st to Die (Women's Murder Club, #1)\n",
      "- Blood Work (Harry Bosch Universe, #8; Terry McCaleb #1)\n",
      "- Chasing the Dime\n",
      "- Void Moon\n",
      "- Murder at the Vicarage (Miss Marple, #1)\n",
      "- The Seven-Percent Solution\n",
      "- Don't Blink\n",
      "- The Ice Princess (Patrik Hedström, #1)\n"
     ]
    }
   ],
   "source": [
    "# Visualizar las recomendaciones\n",
    "print(\"Recomendaciones para el usuario 1:\")\n",
    "for book in recommended_books[:10]:\n",
    "    print(\"-\", book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_model.save('recommender_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendador",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
